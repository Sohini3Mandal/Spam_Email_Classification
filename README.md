# ğŸ“§ Spam Email Classification

An end-to-end spam email classification project using **Natural Language Processing (NLP)** and **classical machine learning algorithms**.  
The project focuses on achieving **high precision** to minimize false positives while maintaining strong overall performance.

---

## ğŸ“‚ Repository Structure

```

Spam_Email_Classification/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ SpamEmailClassificationNotebook.ipynb
â”œâ”€â”€ SpamEmailClassificationReport.pdf
â”œâ”€â”€ emails.csv

```

---

## ğŸ¯ Objective

To build and evaluate machine learning models for classifying email messages as **Spam** or **Ham (Not Spam)** based on their textual content.  
Precision is prioritized because misclassifying legitimate emails as spam is more costly than allowing some spam emails to pass through.

---

## ğŸ“Š Dataset Information

- **Dataset Name:** Spam Email Dataset  
- **Source:** Kaggle 
- **Target Variable:**
  - `1` â†’ Spam  
  - `0` â†’ Ham  

### Initial Columns
- `text`: Raw email content  
- `spam`: Spam label  

### Data Cleaning Summary
- Retained only relevant columns
- Removed missing and duplicate values
- Removed invalid labels
- Final dataset size: **5,693 emails**
- Converted labels to numeric format

The dataset is **imbalanced**, with ham emails significantly outnumbering spam emails.

---

## ğŸ” Exploratory Data Analysis (EDA)

- Engineered length-based features:
  - Number of characters
  - Number of words
  - Number of sentences
- Spam and ham emails show **positively skewed distributions**
- Significant overlap exists between classes
- Strong correlation among length features
- No strong linear relationship between email length and spam label

Conclusion: **Length-based features alone are insufficient for classification**

---

## ğŸ§¹ Text Preprocessing

The following preprocessing steps were applied:
- Removal of subject prefixes
- Lowercasing
- Tokenization (NLTK)
- Removal of non-alphanumeric tokens
- Stopword and punctuation removal
- Stemming using Porter Stemmer

A new column `transformed_text` was created for modeling.

---

## ğŸ§  Feature Extraction

- **TF-IDF Vectorization**
  - Maximum features: 3000
  - Vectorizer fitted only on training data to prevent data leakage
- **Minâ€“Max Scaling** applied to TF-IDF features

---

## ğŸ¤– Models Trained

- Gaussian Naive Bayes  
- Multinomial Naive Bayes  
- Bernoulli Naive Bayes  
- Complement Naive Bayes  
- Logistic Regression  
- Support Vector Classifier  
- K-Nearest Neighbors  
- Decision Tree  
- Random Forest  
- AdaBoost  
- Bagging  
- Extra Trees  
- Gradient Boosting  
- XGBoost  

Hyperparameter tuning was performed using **GridSearchCV** with **Stratified K-Fold Cross-Validation (k = 4)**.  
**Precision** was used as the primary scoring metric.

---

## ğŸ”— Ensemble Models

### Voting Classifier
- Base models: Multinomial NB, Gradient Boosting, XGBoost  
- Soft voting strategy  

### Stacking Classifier
- Base learners: Multinomial NB, Gradient Boosting, XGBoost  
- Meta-learner: Logistic Regression  

---

## ğŸ† Final Model Performance

**Stacking Classifier**
- Accuracy: **0.9895**
- Precision: **0.9712**
- F1-score: **0.9783**
- ROCâ€“AUC: **â‰ˆ 1.00**

The ROC curve lies close to the top-left corner, indicating strong discriminative capability.

---

## ğŸ› ï¸ Tools & Libraries Used

- **Python**
- NumPy
- Pandas
- Matplotlib
- Seaborn
- NLTK
- Scikit-learn
- XGBoost
- WordCloud

---

## ğŸ”® Conclusion

The Stacking Classifier achieved the best overall performance by effectively balancing precision and recall. Due to class imbalance, accuracy alone was found to be misleading; therefore, precision and F1-score were emphasized for model comparison.
While TF-IDF-based models perform well, they ignore word order and semantic context, rely on a fixed vocabulary, and do not handle concept drift or multilingual data. Important metadata such as email headers and URLs were not incorporated.

Future work may explore alternative embeddings, deep learning models, improved imbalance handling, and adaptive learning strategies.

---
